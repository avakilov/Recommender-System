{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc1ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. orders.csv Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3421083 entries, 0 to 3421082\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   order_id                int64  \n",
      " 1   user_id                 int64  \n",
      " 2   eval_set                object \n",
      " 3   order_number            int64  \n",
      " 4   order_dow               int64  \n",
      " 5   order_hour_of_day       int64  \n",
      " 6   days_since_prior_order  float64\n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 182.7+ MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   2539329        1    prior             1          2                  8   \n",
      "1   2398795        1    prior             2          3                  7   \n",
      "2    473747        1    prior             3          3                 12   \n",
      "3   2254736        1    prior             4          4                  7   \n",
      "4    431534        1    prior             5          4                 15   \n",
      "\n",
      "   days_since_prior_order  \n",
      "0                     NaN  \n",
      "1                    15.0  \n",
      "2                    21.0  \n",
      "3                    29.0  \n",
      "4                    28.0  \n",
      "----------------------------------------\n",
      "--- 2. order_products_prior.csv Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Dtype\n",
      "---  ------             -----\n",
      " 0   order_id           int64\n",
      " 1   product_id         int64\n",
      " 2   add_to_cart_order  int64\n",
      " 3   reordered          int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 989.8 MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "   order_id  product_id  add_to_cart_order  reordered\n",
      "0         2       33120                  1          1\n",
      "1         2       28985                  2          1\n",
      "2         2        9327                  3          0\n",
      "3         2       45918                  4          1\n",
      "4         2       30035                  5          0\n",
      "----------------------------------------\n",
      "--- 3. products.csv Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49688 entries, 0 to 49687\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   product_id     49688 non-null  int64 \n",
      " 1   product_name   49688 non-null  object\n",
      " 2   aisle_id       49688 non-null  int64 \n",
      " 3   department_id  49688 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "   product_id                                       product_name  aisle_id  \\\n",
      "0           1                         Chocolate Sandwich Cookies        61   \n",
      "1           2                                   All-Seasons Salt       104   \n",
      "2           3               Robust Golden Unsweetened Oolong Tea        94   \n",
      "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
      "4           5                          Green Chile Anytime Sauce         5   \n",
      "\n",
      "   department_id  \n",
      "0             19  \n",
      "1             13  \n",
      "2              7  \n",
      "3              1  \n",
      "4             13  \n",
      "----------------------------------------\n",
      "--- 4. Data Magnitudes ---\n",
      "Total number of unique users: 206209\n",
      "Total number of orders (prior/train/test combined): 3421083\n",
      "Total number of unique products: 49688\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = 'data'\n",
    "\n",
    "# --- Load the Key DataFrames ---\n",
    "try:\n",
    "    orders_df = pd.read_csv(os.path.join(data_folder, 'orders.csv'))\n",
    "    order_products_prior_df = pd.read_csv(os.path.join(data_folder, 'order_products_prior.csv'))\n",
    "    products_df = pd.read_csv(os.path.join(data_folder, 'products.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file. Check the folder name ('data') and file names.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Display Required Information ---\n",
    "\n",
    "print(\"--- 1. orders.csv Info ---\")\n",
    "print(orders_df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(orders_df.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"--- 2. order_products_prior.csv Info ---\")\n",
    "print(order_products_prior_df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(order_products_prior_df.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"--- 3. products.csv Info ---\")\n",
    "print(products_df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(products_df.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Optional: Display counts for orders.csv to see user/order magnitude\n",
    "print(\"--- 4. Data Magnitudes ---\")\n",
    "print(f\"Total number of unique users: {orders_df['user_id'].nunique()}\")\n",
    "print(f\"Total number of orders (prior/train/test combined): {len(orders_df)}\")\n",
    "print(f\"Total number of unique products: {products_df['product_id'].nunique()}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660eb0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Merge Complete ---\n",
      "Combined DataFrame shape (prior_products_df): (32434489, 9)\n",
      "   order_id  product_id  add_to_cart_order  reordered           product_name  \\\n",
      "0         2       33120                  1          1     Organic Egg Whites   \n",
      "1         2       28985                  2          1  Michigan Organic Kale   \n",
      "2         2        9327                  3          0          Garlic Powder   \n",
      "3         2       45918                  4          1         Coconut Butter   \n",
      "4         2       30035                  5          0      Natural Sweetener   \n",
      "\n",
      "   aisle_id  department_id               aisle  department  \n",
      "0        86             16                eggs  dairy eggs  \n",
      "1        83              4    fresh vegetables     produce  \n",
      "2       104             13   spices seasonings      pantry  \n",
      "3        19             13       oils vinegars      pantry  \n",
      "4        17             13  baking ingredients      pantry  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "# Load the key files\n",
    "try:\n",
    "    orders_df = pd.read_csv(os.path.join(data_folder, 'orders.csv'))\n",
    "    order_products_prior_df = pd.read_csv(os.path.join(data_folder, 'order_products_prior.csv'))\n",
    "    products_df = pd.read_csv(os.path.join(data_folder, 'products.csv'))\n",
    "    aisles_df = pd.read_csv(os.path.join(data_folder, 'aisles.csv')) # Need these for readable names\n",
    "    departments_df = pd.read_csv(os.path.join(data_folder, 'departments.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure all files (including aisles.csv and departments.csv) are in the 'data' folder.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Merge product details (product, aisle, department)\n",
    "product_details_df = products_df.merge(aisles_df, on='aisle_id', how='left')\n",
    "product_details_df = product_details_df.merge(departments_df, on='department_id', how='left')\n",
    "\n",
    "# 2. Merge order products with product details\n",
    "prior_products_df = order_products_prior_df.merge(product_details_df, on='product_id', how='left')\n",
    "\n",
    "# 3. Merge with order information (optional for this analysis, but good practice)\n",
    "# We only need the order_id, product_id, and details for this task.\n",
    "# Let's keep the merge simple for now to focus on product popularity.\n",
    "\n",
    "print(\"--- Data Merge Complete ---\")\n",
    "print(f\"Combined DataFrame shape (prior_products_df): {prior_products_df.shape}\")\n",
    "print(prior_products_df.head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc1792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ðŸ¥‡ Top 10 Most Popular Products\n",
      "             product_name   count\n",
      "0                  Banana  472565\n",
      "1  Bag of Organic Bananas  379450\n",
      "2    Organic Strawberries  264683\n",
      "3    Organic Baby Spinach  241921\n",
      "4    Organic Hass Avocado  213584\n",
      "5         Organic Avocado  176815\n",
      "6             Large Lemon  152657\n",
      "7            Strawberries  142951\n",
      "8                   Limes  140627\n",
      "9      Organic Whole Milk  137905\n",
      "\n",
      "\n",
      "## ðŸ›’ Top 10 Most Popular Aisles\n",
      "                      aisle_name    count\n",
      "0                   fresh fruits  3642188\n",
      "1               fresh vegetables  3418021\n",
      "2     packaged vegetables fruits  1765313\n",
      "3                         yogurt  1452343\n",
      "4                packaged cheese   979763\n",
      "5                           milk   891015\n",
      "6  water seltzer sparkling water   841533\n",
      "7                 chips pretzels   722470\n",
      "8                soy lactosefree   638253\n",
      "9                          bread   584834\n",
      "\n",
      "\n",
      "## ðŸ¬ Top 10 Most Popular Departments\n",
      "   department_name    count\n",
      "0          produce  9479291\n",
      "1       dairy eggs  5414016\n",
      "2           snacks  2887550\n",
      "3        beverages  2690129\n",
      "4           frozen  2236432\n",
      "5           pantry  1875577\n",
      "6           bakery  1176787\n",
      "7     canned goods  1068058\n",
      "8             deli  1051249\n",
      "9  dry goods pasta   866627\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 2A. Analyze Product Popularity ---\n",
    "\n",
    "# Calculate total orders for each product name\n",
    "product_counts = prior_products_df['product_name'].value_counts().reset_index()\n",
    "product_counts.columns = ['product_name', 'count']\n",
    "\n",
    "print(\"## ðŸ¥‡ Top 10 Most Popular Products\")\n",
    "print(product_counts.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate total orders for each aisle\n",
    "aisle_counts = prior_products_df['aisle'].value_counts().reset_index()\n",
    "aisle_counts.columns = ['aisle_name', 'count']\n",
    "\n",
    "print(\"## ðŸ›’ Top 10 Most Popular Aisles\")\n",
    "print(aisle_counts.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate total orders for each department\n",
    "department_counts = prior_products_df['department'].value_counts().reset_index()\n",
    "department_counts.columns = ['department_name', 'count']\n",
    "\n",
    "print(\"## ðŸ¬ Top 10 Most Popular Departments\")\n",
    "print(department_counts.head(10))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12918ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ðŸ”„ Product Reorder Rate Analysis (3B)\n",
      "Reorder rate for the 10 most popular products:\n",
      "                 product_name  total_reordered  total_purchased  reorder_rate\n",
      "3676                   Banana           398609           472565      0.843501\n",
      "3471   Bag of Organic Bananas           315913           379450      0.832555\n",
      "32478      Organic Whole Milk           114510           137905      0.830354\n",
      "30297    Organic Hass Avocado           170131           213584      0.796553\n",
      "31920    Organic Strawberries           205845           264683      0.777704\n",
      "28840    Organic Baby Spinach           186884           241921      0.772500\n",
      "28804         Organic Avocado           134044           176815      0.758103\n",
      "42904            Strawberries            99802           142951      0.698155\n",
      "22413             Large Lemon           106255           152657      0.696038\n",
      "23420                   Limes            95768           140627      0.681007\n",
      "--------------------------------------------------\n",
      "## ðŸ›’ Cart Position vs. Reorder Rate (3C)\n",
      "Average reorder rate as a function of its position in the cart:\n",
      "    add_to_cart_order  average_reorder_rate\n",
      "0                   1              0.677533\n",
      "1                   2              0.676251\n",
      "2                   3              0.658037\n",
      "3                   4              0.636958\n",
      "4                   5              0.617383\n",
      "5                   6              0.600420\n",
      "6                   7              0.585687\n",
      "7                   8              0.573247\n",
      "8                   9              0.561474\n",
      "9                  10              0.551018\n",
      "10                 11              0.541014\n",
      "11                 12              0.532583\n",
      "12                 13              0.524776\n",
      "13                 14              0.516375\n",
      "14                 15              0.509190\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Assuming prior_products_df is available from the previous step ---\n",
    "\n",
    "# 1. Prepare Reorder Metrics at the Product Level\n",
    "reorder_metrics = prior_products_df.groupby('product_name')['reordered'].agg(['sum', 'count']).reset_index()\n",
    "reorder_metrics.columns = ['product_name', 'total_reordered', 'total_purchased']\n",
    "reorder_metrics['reorder_rate'] = reorder_metrics['total_reordered'] / reorder_metrics['total_purchased']\n",
    "\n",
    "# Filter to get metrics for the 10 most popular products you listed\n",
    "top_10_names = product_counts.head(10)['product_name'].tolist()\n",
    "top_10_reorder_rate = reorder_metrics[reorder_metrics['product_name'].isin(top_10_names)]\n",
    "top_10_reorder_rate = top_10_reorder_rate.sort_values('reorder_rate', ascending=False)\n",
    "\n",
    "print(\"## ðŸ”„ Product Reorder Rate Analysis (3B)\")\n",
    "print(\"Reorder rate for the 10 most popular products:\")\n",
    "print(top_10_reorder_rate)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 2. Analyze Reorder Rate by Add-to-Cart Position\n",
    "# Group by add_to_cart_order and calculate the average reorder rate\n",
    "cart_position_analysis = prior_products_df.groupby('add_to_cart_order')['reordered'].mean().reset_index()\n",
    "cart_position_analysis.columns = ['add_to_cart_order', 'average_reorder_rate']\n",
    "\n",
    "# Display the first 15 positions, as most carts are short\n",
    "print(\"## ðŸ›’ Cart Position vs. Reorder Rate (3C)\")\n",
    "print(\"Average reorder rate as a function of its position in the cart:\")\n",
    "print(cart_position_analysis.head(15))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8ffca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal EDA plots saved: temporal_patterns_1a.png (Day/Hour) and temporal_patterns_1b.png (Days Since Prior Order).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = 'data'\n",
    "\n",
    "# Load the orders DataFrame\n",
    "try:\n",
    "    orders_df = pd.read_csv(os.path.join(data_folder, 'orders.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure orders.csv is in the 'data' folder.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------- 1A. Shopping Schedule -----------------\n",
    "\n",
    "# Count orders by Day of Week (order_dow) and Hour of Day (order_hour_of_day)\n",
    "dow_counts = orders_df['order_dow'].value_counts().sort_index()\n",
    "hour_counts = orders_df['order_hour_of_day'].value_counts().sort_index()\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Orders by Day of Week\n",
    "dow_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Orders by Day of Week', fontsize=14)\n",
    "axes[0].set_xlabel('Day of Week (0=Sun, 6=Sat)', fontsize=12)\n",
    "axes[0].set_ylabel('Total Orders (Millions)', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1e6:0.1f}M'))\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Orders by Hour of Day\n",
    "hour_counts.plot(kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('Orders by Hour of Day', fontsize=14)\n",
    "axes[1].set_xlabel('Hour of Day (0-23)', fontsize=12)\n",
    "axes[1].set_ylabel('Total Orders (Millions)', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1e6:0.1f}M'))\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('temporal_patterns_1a.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ----------------- 1B. Order Gap Analysis -----------------\n",
    "\n",
    "# Filter out the first order for each user (where days_since_prior_order is NaN)\n",
    "days_since_prior_order = orders_df['days_since_prior_order'].dropna()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(9, 6))\n",
    "# Use fixed bins from 0 to 30 (the maximum value is usually 30)\n",
    "plt.hist(days_since_prior_order, bins=np.arange(0.5, 31.5, 1), color='mediumseagreen', edgecolor='black')\n",
    "plt.title('Distribution of Days Since Prior Order', fontsize=14)\n",
    "plt.xlabel('Days Since Prior Order', fontsize=12)\n",
    "plt.ylabel('Number of Orders (Millions)', fontsize=12)\n",
    "plt.xticks(np.arange(1, 31, 1)) # Label every day\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1e6:0.1f}M'))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('temporal_patterns_1b.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Temporal EDA plots saved: temporal_patterns_1a.png (Day/Hour) and temporal_patterns_1b.png (Days Since Prior Order).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac79843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Calculating User Features...\n",
      "User features created for 206209 users.\n",
      "   user_id  u_mean_days_since_prior  u_total_orders  u_reorder_rate\n",
      "0        1                19.555556              10        0.694915\n",
      "1        2                15.230769              14        0.476923\n",
      "2        3                12.090909              12        0.625000\n",
      "3        4                13.750000               5        0.055556\n",
      "4        5                13.333333               4        0.378378\n",
      "--------------------------------------------------\n",
      "Calculating Product Features...\n",
      "Product features created for 49677 products.\n",
      "   product_id  p_total_purchased  p_reorder_rate\n",
      "0           1               1852        0.613391\n",
      "1           2                 90        0.133333\n",
      "2           3                277        0.732852\n",
      "3           4                329        0.446809\n",
      "4           5                 15        0.600000\n",
      "--------------------------------------------------\n",
      "Calculating User-Product Interaction Features (This may take a moment)...\n",
      "User-Product features created for 13307953 unique pairs.\n",
      "   user_id  product_id  up_total_purchases  up_last_order_number  \\\n",
      "0        1         196                  10                    10   \n",
      "1        1       10258                   9                    10   \n",
      "2        1       10326                   1                     5   \n",
      "3        1       12427                  10                    10   \n",
      "4        1       13032                   3                    10   \n",
      "\n",
      "   up_mean_add_to_cart  up_order_gap  \n",
      "0             1.400000             0  \n",
      "1             3.333333             0  \n",
      "2             5.000000             5  \n",
      "3             3.300000             0  \n",
      "4             6.333333             0  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder = 'data'\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load necessary dataframes\n",
    "try:\n",
    "    orders_df = pd.read_csv(os.path.join(data_folder, 'orders.csv'))\n",
    "    op_prior = pd.read_csv(os.path.join(data_folder, 'order_products_prior.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Filter orders to only include 'prior' set for feature engineering\n",
    "orders_prior_df = orders_df[orders_df['eval_set'] == 'prior']\n",
    "\n",
    "# --- A. User Habit Features (User-Level) ---\n",
    "print(\"Calculating User Features...\")\n",
    "\n",
    "# Calculate mean days since prior order and total orders\n",
    "user_features = orders_prior_df.groupby('user_id').agg(\n",
    "    u_mean_days_since_prior=('days_since_prior_order', 'mean'),\n",
    "    u_total_orders=('order_number', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate overall user reorder rate\n",
    "# Need to merge order info with order_products to get user_id for every item\n",
    "user_reorder_df = op_prior.merge(orders_prior_df[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "user_reorder_rate = user_reorder_df.groupby('user_id')['reordered'].agg(\n",
    "    u_reorder_sum='sum',\n",
    "    u_reorder_count='count'\n",
    ").reset_index()\n",
    "user_reorder_rate['u_reorder_rate'] = user_reorder_rate['u_reorder_sum'] / user_reorder_rate['u_reorder_count']\n",
    "user_reorder_rate = user_reorder_rate[['user_id', 'u_reorder_rate']]\n",
    "\n",
    "# Final user feature set\n",
    "user_features = user_features.merge(user_reorder_rate, on='user_id', how='left')\n",
    "print(f\"User features created for {len(user_features)} users.\")\n",
    "print(user_features.head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- B. Product Habit Features (Product-Level) ---\n",
    "print(\"Calculating Product Features...\")\n",
    "\n",
    "# Calculate total purchases and product reorder rate\n",
    "product_features = op_prior.groupby('product_id')['reordered'].agg(\n",
    "    p_total_purchased='count',\n",
    "    p_reorder_sum='sum'\n",
    ").reset_index()\n",
    "\n",
    "product_features['p_reorder_rate'] = product_features['p_reorder_sum'] / product_features['p_total_purchased']\n",
    "product_features = product_features[['product_id', 'p_total_purchased', 'p_reorder_rate']]\n",
    "\n",
    "print(f\"Product features created for {len(product_features)} products.\")\n",
    "print(product_features.head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- C. User-Product Interaction Features (Linker Features) ---\n",
    "print(\"Calculating User-Product Interaction Features (This may take a moment)...\")\n",
    "\n",
    "# Join orders to order_products_prior to get the order_number for each purchase\n",
    "up_df = op_prior.merge(orders_prior_df[['order_id', 'user_id', 'order_number']], on='order_id', how='left')\n",
    "\n",
    "# Calculate the critical user-product features\n",
    "up_features = up_df.groupby(['user_id', 'product_id']).agg(\n",
    "    up_total_purchases=('order_id', 'count'),\n",
    "    up_last_order_number=('order_number', 'max'),\n",
    "    up_mean_add_to_cart=('add_to_cart_order', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge with the full user feature set to get the maximum order number for that user\n",
    "up_features = up_features.merge(\n",
    "    orders_prior_df.groupby('user_id')['order_number'].max().rename('u_max_order_number'),\n",
    "    on='user_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate the crucial 'up_order_gap' feature\n",
    "up_features['up_order_gap'] = up_features['u_max_order_number'] - up_features['up_last_order_number']\n",
    "\n",
    "up_features = up_features[['user_id', 'product_id', 'up_total_purchases', \n",
    "                           'up_last_order_number', 'up_mean_add_to_cart', 'up_order_gap']]\n",
    "\n",
    "print(f\"User-Product features created for {len(up_features)} unique pairs.\")\n",
    "print(up_features.head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25583d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸŽ¯ Creating Final Training Dataset ---\n",
      "Total number of positive reorder events in the train set: 1384617\n",
      "\n",
      "Final Training Dataset Sample (Features + Target):\n",
      "   user_id  product_id  up_total_purchases  up_last_order_number  \\\n",
      "0        1         196                  10                    10   \n",
      "1        1       10258                   9                    10   \n",
      "2        1       10326                   1                     5   \n",
      "3        1       12427                  10                    10   \n",
      "4        1       13032                   3                    10   \n",
      "\n",
      "   up_mean_add_to_cart  up_order_gap  p_total_purchased  p_reorder_rate  \\\n",
      "0             1.400000             0              35791        0.776480   \n",
      "1             3.333333             0               1946        0.713772   \n",
      "2             5.000000             5               5526        0.652009   \n",
      "3             3.300000             0               6476        0.740735   \n",
      "4             6.333333             0               3751        0.657158   \n",
      "\n",
      "   u_mean_days_since_prior  u_total_orders  u_reorder_rate  reordered_next  \n",
      "0                19.555556              10        0.694915               1  \n",
      "1                19.555556              10        0.694915               1  \n",
      "2                19.555556              10        0.694915               0  \n",
      "3                19.555556              10        0.694915               0  \n",
      "4                19.555556              10        0.694915               1  \n",
      "\n",
      "Final Training Dataset Shape: (13307953, 12)\n",
      "Proportion of Positive Cases (Y=1): 0.0623\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Assuming user_features, product_features, and up_features are available ---\n",
    "\n",
    "print(\"--- ðŸŽ¯ Creating Final Training Dataset ---\")\n",
    "\n",
    "# 1. Load the order_products_train.csv file to define the target variable (Y)\n",
    "try:\n",
    "    op_train = pd.read_csv(os.path.join(data_folder, 'order_products_train.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading order_products_train.csv: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Identify the target orders (the train order for each user)\n",
    "orders_train = orders_df[orders_df['eval_set'] == 'train'][['user_id', 'order_id']]\n",
    "\n",
    "# 3. Create the Target Table: A list of (user_id, product_id) pairs that were bought next\n",
    "# Filter op_train to only include the columns we need (order_id, product_id)\n",
    "target_df = orders_train.merge(op_train[['order_id', 'product_id']], on='order_id', how='inner')\n",
    "target_df = target_df[['user_id', 'product_id']].drop_duplicates()\n",
    "\n",
    "# Create a 'reordered_next' column set to 1 for all pairs in the target\n",
    "target_df['reordered_next'] = 1\n",
    "print(f\"Total number of positive reorder events in the train set: {len(target_df)}\")\n",
    "\n",
    "# 4. Combine all features and merge with the target variable\n",
    "# Start with the User-Product candidates\n",
    "train_set = up_features.copy()\n",
    "\n",
    "# Merge in the Product features (p_total_purchased, p_reorder_rate)\n",
    "train_set = train_set.merge(product_features, on='product_id', how='left')\n",
    "\n",
    "# Merge in the User features (u_mean_days_since_prior, u_total_orders, u_reorder_rate)\n",
    "train_set = train_set.merge(user_features, on='user_id', how='left')\n",
    "\n",
    "# Merge with the Target table to assign the Y=1 labels\n",
    "train_set = train_set.merge(target_df, on=['user_id', 'product_id'], how='left')\n",
    "\n",
    "# Fill NaN values in the target column with 0 (meaning the product was NOT reordered next)\n",
    "train_set['reordered_next'] = train_set['reordered_next'].fillna(0).astype(int)\n",
    "\n",
    "# 5. Final check of the combined training data\n",
    "print(\"\\nFinal Training Dataset Sample (Features + Target):\")\n",
    "print(train_set.head())\n",
    "print(f\"\\nFinal Training Dataset Shape: {train_set.shape}\")\n",
    "print(f\"Proportion of Positive Cases (Y=1): {train_set['reordered_next'].mean():.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548b64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Training Set Size (before subsampling): 10,646,362 rows\n",
      "Validation Set Size: 2,661,591 rows\n",
      "--------------------------------------------------\n",
      "Performing class subsampling...\n",
      "Positive Cases in Training: 663,059 rows\n",
      "Negative Cases Sampled: 663,059 rows\n",
      "Balanced Training Set Size (Final X_train): 1,326,118 rows\n",
      "Validation Set Size (Unbalanced, for realistic evaluation): 2,661,591 rows\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Assuming the 'train_set' DataFrame from the previous step is loaded ---\n",
    "# Note: In a real environment, you'd reload the saved train_set DataFrame here.\n",
    "# For continuity, we assume it's still in memory.\n",
    "\n",
    "# Define Features (X) and Target (Y)\n",
    "features = [\n",
    "    'up_total_purchases', 'up_last_order_number', 'up_mean_add_to_cart', 'up_order_gap',\n",
    "    'u_mean_days_since_prior', 'u_total_orders', 'u_reorder_rate',\n",
    "    'p_total_purchased', 'p_reorder_rate'\n",
    "]\n",
    "X = train_set[features]\n",
    "Y = train_set['reordered_next']\n",
    "df_ids = train_set[['user_id', 'product_id']] # Keep IDs separate for later mapping\n",
    "\n",
    "# 1. Perform Train/Validation Split (80/20)\n",
    "# We split the indices to keep the full dataset structure intact initially\n",
    "# Stratify=Y ensures the proportion of Y=1 cases is maintained in both sets.\n",
    "X_train_full, X_val, Y_train_full, Y_val, ids_train_full, ids_val = train_test_split(\n",
    "    X, Y, df_ids, test_size=0.20, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "print(f\"Full Training Set Size (before subsampling): {X_train_full.shape[0]:,} rows\")\n",
    "print(f\"Validation Set Size: {X_val.shape[0]:,} rows\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Subsample the Training Data for Balance\n",
    "print(\"Performing class subsampling...\")\n",
    "\n",
    "# Separate the positive and negative cases in the full training set\n",
    "X_train_pos = X_train_full[Y_train_full == 1]\n",
    "Y_train_pos = Y_train_full[Y_train_full == 1]\n",
    "\n",
    "X_train_neg = X_train_full[Y_train_full == 0]\n",
    "Y_train_neg = Y_train_full[Y_train_full == 0]\n",
    "\n",
    "# Sample the negative class to match the size of the positive class\n",
    "sample_size = len(X_train_pos)\n",
    "X_train_neg_sampled = X_train_neg.sample(n=sample_size, random_state=42)\n",
    "Y_train_neg_sampled = Y_train_neg.loc[X_train_neg_sampled.index]\n",
    "\n",
    "# Combine the positive and sampled negative cases to form the final balanced training set\n",
    "X_train_balanced = pd.concat([X_train_pos, X_train_neg_sampled]).sample(frac=1, random_state=42) # Shuffle\n",
    "Y_train_balanced = pd.concat([Y_train_pos, Y_train_neg_sampled]).loc[X_train_balanced.index]\n",
    "\n",
    "print(f\"Positive Cases in Training: {len(X_train_pos):,} rows\")\n",
    "print(f\"Negative Cases Sampled: {len(X_train_neg_sampled):,} rows\")\n",
    "print(f\"Balanced Training Set Size (Final X_train): {X_train_balanced.shape[0]:,} rows\")\n",
    "print(f\"Validation Set Size (Unbalanced, for realistic evaluation): {X_val.shape[0]:,} rows\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3daa039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ‹ï¸ Training XGBoost Model ---\n",
      "Training complete.\n",
      "--------------------------------------------------\n",
      "Predicting probabilities on validation set...\n",
      "Validation F1 Score (at threshold=0.50): 0.2604\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Assuming X_train_balanced, Y_train_balanced, X_val, Y_val are available from the previous step ---\n",
    "\n",
    "print(\"--- ðŸ‹ï¸ Training XGBoost Model ---\")\n",
    "\n",
    "# Define XGBoost parameters\n",
    "# Setting objective='binary:logistic' for binary classification\n",
    "# Setting n_jobs=-1 to use all available cores\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',  # Logloss is standard for training optimization\n",
    "    'eta': 0.1,                # Learning rate\n",
    "    'max_depth': 6,            # Tree depth\n",
    "    'subsample': 0.7,          # Fraction of samples to be randomly samples for each tree\n",
    "    'colsample_bytree': 0.7,   # Fraction of columns to be randomly sampled for each tree\n",
    "    'n_estimators': 100,       # Number of boosting rounds (trees)\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0             # Suppress verbose output\n",
    "}\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = xgb.XGBClassifier(**xgb_params)\n",
    "clf.fit(X_train_balanced, Y_train_balanced)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- ðŸ§ª Predicting and Evaluating on Validation Set ---\n",
    "print(\"Predicting probabilities on validation set...\")\n",
    "\n",
    "# Predict probabilities of Y=1 (the positive class)\n",
    "# We need probabilities to find the optimal F1 threshold\n",
    "y_val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# To calculate F1 score, we need to convert probabilities to binary predictions (0 or 1).\n",
    "# We must choose a threshold. Since the training data was balanced (50/50),\n",
    "# a threshold of 0.5 is a good starting point, but we can tune it later.\n",
    "threshold = 0.5\n",
    "y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(Y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation F1 Score (at threshold={threshold:.2f}): {f1:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc4220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- âš™ï¸ Optimizing F1 Threshold ---\n",
      "Initial F1 Score (t=0.50): 0.2604\n",
      "Optimal Threshold Found: 0.5000\n",
      "Maximum F1 Score Achieved: 0.2604\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Assuming y_val_proba (predicted probabilities) and Y_val (true labels) are available ---\n",
    "\n",
    "print(\"--- âš™ï¸ Optimizing F1 Threshold ---\")\n",
    "\n",
    "# Define a range of thresholds to check\n",
    "# We use 100 points between 0.01 and 0.50\n",
    "thresholds = np.linspace(0.01, 0.50, 100)\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate through thresholds and calculate F1 score\n",
    "for t in thresholds:\n",
    "    # Convert probability to binary prediction using threshold t\n",
    "    y_val_pred_t = (y_val_proba >= t).astype(int)\n",
    "    f1 = f1_score(Y_val, y_val_pred_t)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the threshold that produced the maximum F1 score\n",
    "max_f1_index = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[max_f1_index]\n",
    "max_f1_score = f1_scores[max_f1_index]\n",
    "\n",
    "print(f\"Initial F1 Score (t=0.50): {f1_scores[-1]:.4f}\")\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold:.4f}\")\n",
    "print(f\"Maximum F1 Score Achieved: {max_f1_score:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ccd8dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“Š Feature Importance Analysis ---\n",
      "                   Feature  Importance\n",
      "3             up_order_gap    0.585263\n",
      "0       up_total_purchases    0.218454\n",
      "5           u_total_orders    0.058936\n",
      "8           p_reorder_rate    0.053298\n",
      "1     up_last_order_number    0.035517\n",
      "7        p_total_purchased    0.019498\n",
      "6           u_reorder_rate    0.015059\n",
      "4  u_mean_days_since_prior    0.007793\n",
      "2      up_mean_add_to_cart    0.006184\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Assuming clf (the trained XGBoost model) and features (list of feature names) are available ---\n",
    "\n",
    "print(\"--- ðŸ“Š Feature Importance Analysis ---\")\n",
    "\n",
    "# Extract feature importances\n",
    "importance = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "# Sort by importance and display top features\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(9))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ccb92e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ”¬ Retraining XGBoost with Tuned Parameters ---\n",
      "Tuned training complete.\n",
      "--------------------------------------------------\n",
      "Re-evaluating F1 Score with threshold optimization...\n",
      "Initial F1 Score (t=0.50): 0.2609\n",
      "Optimal Threshold Found: 0.5000\n",
      "Maximum F1 Score Achieved (Tuned Model): 0.2609\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Assuming X_train_balanced, Y_train_balanced, X_val, Y_val are available ---\n",
    "\n",
    "print(\"--- ðŸ”¬ Retraining XGBoost with Tuned Parameters ---\")\n",
    "\n",
    "# Define NEW XGBoost parameters\n",
    "xgb_params_tuned = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,            # Slower learning rate\n",
    "    'max_depth': 8,         # Deeper trees\n",
    "    'n_estimators': 200,    # More trees (to compensate for slower eta)\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Initialize and train the tuned classifier\n",
    "clf_tuned = xgb.XGBClassifier(**xgb_params_tuned)\n",
    "clf_tuned.fit(X_train_balanced, Y_train_balanced)\n",
    "\n",
    "print(\"Tuned training complete.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- ðŸ§ª Evaluating on Validation Set with Optimized F1 Search ---\n",
    "print(\"Re-evaluating F1 Score with threshold optimization...\")\n",
    "\n",
    "y_val_proba_tuned = clf_tuned.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Re-run the optimization loop to find the best F1 for the new model\n",
    "thresholds = np.linspace(0.01, 0.50, 100)\n",
    "f1_scores = []\n",
    "for t in thresholds:\n",
    "    y_val_pred_t = (y_val_proba_tuned >= t).astype(int)\n",
    "    f1 = f1_score(Y_val, y_val_pred_t)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "max_f1_index = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[max_f1_index]\n",
    "max_f1_score_tuned = f1_scores[max_f1_index]\n",
    "\n",
    "print(f\"Initial F1 Score (t=0.50): {f1_scores[-1]:.4f}\")\n",
    "print(f\"Optimal Threshold Found: {optimal_threshold:.4f}\")\n",
    "print(f\"Maximum F1 Score Achieved (Tuned Model): {max_f1_score_tuned:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c01f8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic Score calculated.\n",
      "   user_id  product_id  up_total_purchases  up_order_gap  heuristic_score\n",
      "0        1         196                  10             0        10.000000\n",
      "1        1       10258                   9             0         9.000000\n",
      "2        1       10326                   1             5         0.166667\n",
      "3        1       12427                  10             0        10.000000\n",
      "4        1       13032                   3             0         3.000000\n",
      "--------------------------------------------------\n",
      "Calculated true order size for 78514 validation users.\n",
      "Validation set size after dropping users with no train-set order: 1,237,716 rows\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Assuming the full 'train_set' from the feature engineering step is available ---\n",
    "# We will use the full, original train_set (13.3M rows) for this ranking evaluation\n",
    "\n",
    "# 1. Create the Heuristic Score Feature\n",
    "train_set['heuristic_score'] = train_set['up_total_purchases'] / (train_set['up_order_gap'] + 1)\n",
    "\n",
    "print(\"Heuristic Score calculated.\")\n",
    "print(train_set[['user_id', 'product_id', 'up_total_purchases', 'up_order_gap', 'heuristic_score']].head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 2. Split the data back into train/validation sets (same split as before)\n",
    "# We must use the original, full X and Y before any subsampling.\n",
    "features_full = train_set.columns.tolist() # All columns are needed now\n",
    "X = train_set\n",
    "Y = train_set['reordered_next']\n",
    "\n",
    "# We need the user_id to correctly map orders later.\n",
    "X_train_full, X_val, Y_train_full, Y_val = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# 3. Prepare Validation Data for P@K Calculation\n",
    "# We only need the validation data (X_val) for this step\n",
    "validation_df = X_val.copy()\n",
    "\n",
    "# 4. Map the target (reordered_next) to the validation set\n",
    "# We need to know the true next order size for each user.\n",
    "# For simplicity, we will assume we can re-derive the order size by counting Y=1 cases in the validation set.\n",
    "user_order_size = validation_df[validation_df['reordered_next'] == 1].groupby('user_id')['reordered_next'].count().reset_index()\n",
    "user_order_size.columns = ['user_id', 'true_order_size']\n",
    "print(f\"Calculated true order size for {len(user_order_size)} validation users.\")\n",
    "\n",
    "# 5. Merge the true order size into the validation set\n",
    "validation_df = validation_df.merge(user_order_size, on='user_id', how='left')\n",
    "# Drop users who did not order anything in the final set (true_order_size is NaN)\n",
    "validation_df.dropna(subset=['true_order_size'], inplace=True)\n",
    "validation_df['true_order_size'] = validation_df['true_order_size'].astype(int)\n",
    "\n",
    "print(f\"Validation set size after dropping users with no train-set order: {validation_df.shape[0]:,} rows\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b8a44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ¥‡ Evaluating Heuristic Model: Precision@N ---\n",
      "Total Users Evaluated: 78514\n",
      "Final Average Precision@N Score: 0.4755\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/7pjhf0ls0wlgbbf_w00d7w7m0000gn/T/ipykernel_2893/1948916502.py:39: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_precision_scores = validation_df.groupby('user_id').apply(calculate_precision_at_n)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Assuming validation_df is available from the previous step ---\n",
    "\n",
    "def calculate_precision_at_n(group):\n",
    "    \"\"\"Calculates Precision@N for a single user group.\"\"\"\n",
    "    \n",
    "    # 1. Determine N (the size of the next order)\n",
    "    # Since we are evaluating users who placed a train order, the true_order_size column \n",
    "    # should contain the count of Y=1 items for that user.\n",
    "    N = group['true_order_size'].iloc[0]\n",
    "    \n",
    "    # Check for empty orders or small baskets\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Sort the candidate items by the heuristic score\n",
    "    # We must reset the index after sorting to ensure .iloc works correctly\n",
    "    sorted_group = group.sort_values('heuristic_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # 3. Select the Top N predictions\n",
    "    # This gives us the top N candidates based on our score\n",
    "    top_N_predictions = sorted_group.head(N)\n",
    "    \n",
    "    # 4. Count the True Positives (TP)\n",
    "    # TP are the predicted items (top N) that were actually reordered (reordered_next = 1)\n",
    "    true_positives = top_N_predictions['reordered_next'].sum()\n",
    "    \n",
    "    # 5. Calculate Precision@N\n",
    "    # P@N = TP / N\n",
    "    precision_at_n = true_positives / N\n",
    "    \n",
    "    return precision_at_n\n",
    "\n",
    "print(\"--- ðŸ¥‡ Evaluating Heuristic Model: Precision@N ---\")\n",
    "\n",
    "# Apply the function to every user in the validation set\n",
    "# The result is a Pandas Series where the index is user_id and the value is P@N\n",
    "user_precision_scores = validation_df.groupby('user_id').apply(calculate_precision_at_n)\n",
    "\n",
    "# Calculate the final mean average Precision@N across all users\n",
    "mean_precision_at_n = user_precision_scores.mean()\n",
    "\n",
    "print(f\"Total Users Evaluated: {len(user_precision_scores)}\")\n",
    "print(f\"Final Average Precision@N Score: {mean_precision_at_n:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ed0a0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated new decay score.\n",
      "   user_id  up_order_gap  heuristic_score  decay_factor  heuristic_score_decay\n",
      "1    57286             0         1.000000      1.000000               1.000000\n",
      "2   176700             4         0.200000      0.670320               0.134064\n",
      "5   192439            34         0.057143      0.033373               0.001907\n",
      "7    15448            21         0.090909      0.122456               0.011132\n",
      "8    13640             2         0.666667      0.818731               0.545821\n",
      "--------------------------------------------------\n",
      "--- ðŸ¥‡ Evaluating Time-Decayed Heuristic: Precision@N ---\n",
      "Original Average Precision@N Score: 0.4755\n",
      "New Average Precision@N Score (Decayed): 0.4734\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/7pjhf0ls0wlgbbf_w00d7w7m0000gn/T/ipykernel_2893/3290621650.py:35: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_precision_scores_decay = validation_df.groupby('user_id').apply(calculate_precision_at_n_decay)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Assuming validation_df is available from the previous step ---\n",
    "\n",
    "# Set a decay factor (lambda). Let's start with a mild decay.\n",
    "# A small value (e.g., 0.1) means the score drops by about 10% for each order gap.\n",
    "DECAY_LAMBDA = 0.1\n",
    "\n",
    "# Calculate the new decay score\n",
    "validation_df['decay_factor'] = np.exp(-DECAY_LAMBDA * validation_df['up_order_gap'])\n",
    "validation_df['heuristic_score_decay'] = validation_df['heuristic_score'] * validation_df['decay_factor']\n",
    "\n",
    "print(\"Calculated new decay score.\")\n",
    "print(validation_df[['user_id', 'up_order_gap', 'heuristic_score', 'decay_factor', 'heuristic_score_decay']].head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def calculate_precision_at_n_decay(group):\n",
    "    \"\"\"Calculates Precision@N using the decay score.\"\"\"\n",
    "    N = group['true_order_size'].iloc[0]\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Sort using the new decay score\n",
    "    sorted_group = group.sort_values('heuristic_score_decay', ascending=False).reset_index(drop=True)\n",
    "    top_N_predictions = sorted_group.head(N)\n",
    "    true_positives = top_N_predictions['reordered_next'].sum()\n",
    "    \n",
    "    return true_positives / N\n",
    "\n",
    "print(\"--- ðŸ¥‡ Evaluating Time-Decayed Heuristic: Precision@N ---\")\n",
    "\n",
    "# Apply the new evaluation function\n",
    "user_precision_scores_decay = validation_df.groupby('user_id').apply(calculate_precision_at_n_decay)\n",
    "\n",
    "mean_precision_at_n_decay = user_precision_scores_decay.mean()\n",
    "\n",
    "print(f\"Original Average Precision@N Score: 0.4755\")\n",
    "print(f\"New Average Precision@N Score (Decayed): {mean_precision_at_n_decay:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e33bd87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ”¬ Blending Features for Final Heuristic ---\n",
      "Blended Score calculated.\n",
      "   user_id  up_total_purchases  recency_inverse  p_reorder_rate  \\\n",
      "1    57286                   1         1.000000        0.603591   \n",
      "2   176700                   1         0.200000        0.457077   \n",
      "5   192439                   2         0.028571        0.379371   \n",
      "7    15448                   2         0.045455        0.590539   \n",
      "8    13640                   2         0.333333        0.701996   \n",
      "\n",
      "   heuristic_score_blended  \n",
      "1                 2.603591  \n",
      "2                 1.657077  \n",
      "5                 2.407942  \n",
      "7                 2.635994  \n",
      "8                 3.035329  \n",
      "--------------------------------------------------\n",
      "--- ðŸ¥‡ Evaluating Final Blended Heuristic: Precision@N ---\n",
      "Original Average Precision@N Score: 0.4755\n",
      "New Average Precision@N Score (Blended): 0.4655\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/7pjhf0ls0wlgbbf_w00d7w7m0000gn/T/ipykernel_2893/1950073525.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_precision_scores_blended = validation_df.groupby('user_id').apply(calculate_precision_at_n_blended)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Assuming validation_df is available ---\n",
    "\n",
    "print(\"--- ðŸ”¬ Blending Features for Final Heuristic ---\")\n",
    "\n",
    "# 1. Calculate the three component scores\n",
    "validation_df['recency_inverse'] = 1 / (validation_df['up_order_gap'] + 1)\n",
    "# Note: up_total_purchases and p_reorder_rate are already in validation_df\n",
    "\n",
    "# 2. Calculate the blended score (linear combination)\n",
    "# We give equal weight (1) to the three most predictive factors identified by feature importance.\n",
    "# This creates a combined score that captures frequency, recency, and category quality.\n",
    "validation_df['heuristic_score_blended'] = (\n",
    "    validation_df['up_total_purchases'] +\n",
    "    validation_df['recency_inverse'] +\n",
    "    validation_df['p_reorder_rate']\n",
    ")\n",
    "\n",
    "print(\"Blended Score calculated.\")\n",
    "print(validation_df[['user_id', 'up_total_purchases', 'recency_inverse', 'p_reorder_rate', 'heuristic_score_blended']].head())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def calculate_precision_at_n_blended(group):\n",
    "    \"\"\"Calculates Precision@N using the blended score.\"\"\"\n",
    "    N = group['true_order_size'].iloc[0]\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Sort using the new blended score\n",
    "    sorted_group = group.sort_values('heuristic_score_blended', ascending=False).reset_index(drop=True)\n",
    "    top_N_predictions = sorted_group.head(N)\n",
    "    true_positives = top_N_predictions['reordered_next'].sum()\n",
    "    \n",
    "    return true_positives / N\n",
    "\n",
    "print(\"--- ðŸ¥‡ Evaluating Final Blended Heuristic: Precision@N ---\")\n",
    "\n",
    "# Apply the new evaluation function\n",
    "user_precision_scores_blended = validation_df.groupby('user_id').apply(calculate_precision_at_n_blended)\n",
    "\n",
    "mean_precision_at_n_blended = user_precision_scores_blended.mean()\n",
    "\n",
    "print(f\"Original Average Precision@N Score: 0.4755\")\n",
    "print(f\"New Average Precision@N Score (Blended): {mean_precision_at_n_blended:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db352e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ”¬ Evaluating Precision, Recall, and F1 at 2N ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/7pjhf0ls0wlgbbf_w00d7w7m0000gn/T/ipykernel_2893/628267151.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  metrics_df = validation_df.groupby('user_id').apply(calculate_full_metrics_at_2n).apply(pd.Series)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Precision@N (K=N): 0.4755\n",
      "New Precision@2N (K=2N): 0.3377\n",
      "New Recall@2N (K=2N): 0.6755\n",
      "New F1@2N (K=2N): 0.4503\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Assuming validation_df is available from previous steps ---\n",
    "\n",
    "def calculate_full_metrics_at_2n(group):\n",
    "    \"\"\"Calculates Precision@2N, Recall@2N, and F1@2N for a single user group.\"\"\"\n",
    "    \n",
    "    # N is the true order size\n",
    "    N = group['true_order_size'].iloc[0]\n",
    "    \n",
    "    if N == 0:\n",
    "        return {'Precision@2N': 0.0, 'Recall@2N': 0.0, 'F1@2N': 0.0}\n",
    "\n",
    "    # K is the prediction list size (K = 2 * N)\n",
    "    K = N * 2\n",
    "    \n",
    "    # 1. Sort by the best heuristic score\n",
    "    sorted_group = group.sort_values('heuristic_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # 2. Select the Top K predictions\n",
    "    top_K_predictions = sorted_group.head(K)\n",
    "    \n",
    "    # 3. True Positives (TP)\n",
    "    true_positives = top_K_predictions['reordered_next'].sum()\n",
    "    \n",
    "    # 4. Calculate Metrics\n",
    "    precision_at_k = true_positives / K\n",
    "    recall_at_k = true_positives / N # Denominator is the true order size (N)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision_at_k + recall_at_k == 0:\n",
    "        f1_at_k = 0.0\n",
    "    else:\n",
    "        f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "    \n",
    "    return {'Precision@2N': precision_at_k, 'Recall@2N': recall_at_k, 'F1@2N': f1_at_k}\n",
    "\n",
    "print(\"--- ðŸ”¬ Evaluating Precision, Recall, and F1 at 2N ---\")\n",
    "\n",
    "# Apply the function to every user in the validation set\n",
    "metrics_df = validation_df.groupby('user_id').apply(calculate_full_metrics_at_2n).apply(pd.Series)\n",
    "\n",
    "# Calculate the final mean across all users\n",
    "final_metrics = metrics_df.mean()\n",
    "\n",
    "print(f\"Original Precision@N (K=N): 0.4755\")\n",
    "print(f\"New Precision@2N (K=2N): {final_metrics['Precision@2N']:.4f}\")\n",
    "print(f\"New Recall@2N (K=2N): {final_metrics['Recall@2N']:.4f}\")\n",
    "print(f\"New F1@2N (K=2N): {final_metrics['F1@2N']:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
